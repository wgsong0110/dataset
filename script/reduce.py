import os, sys, torch, numpy as np
from pathlib import Path
from plyfile import PlyData, PlyElement

# src.clustering ëª¨ë“ˆ import
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, str(Path(PROJECT_ROOT) / 'src'))

import clustering as clustering_lib

C0 = 0.28209479177387814  # SH DC ì •ê·œí™” ìƒìˆ˜
COV_EPS = 1e-7  # ê³µë¶„ì‚° ì •ê·œí™” ìƒìˆ˜

# SH â†’ RGB ë³€í™˜
def SH2RGB(sh):
    return sh * C0 + 0.5

# PLY íŒŒì¼ì„ torch tensorë¡œ ë¡œë“œ
def load_ply_as_tensors(ply_path: Path):
    ply = PlyData.read(str(ply_path))
    v = ply["vertex"].data
    xyz = torch.tensor(np.stack([v["x"], v["y"], v["z"]], axis=1), dtype=torch.float32)
    quats = torch.tensor(np.stack([v["rot_0"], v["rot_1"], v["rot_2"], v["rot_3"]], axis=1), dtype=torch.float32)
    scales = torch.tensor(np.stack([v["scale_0"], v["scale_1"], v["scale_2"]], axis=1), dtype=torch.float32)
    scales = torch.exp(scales)
    opacities = torch.tensor(v["opacity"], dtype=torch.float32).unsqueeze(1)
    f_cols = [f for f in v.dtype.names if f.startswith("f_")]
    f_cols.sort(key=lambda s: (0 if "f_dc" in s else 1, int(s.split("_")[-1])))
    features = torch.tensor(np.stack([v[c] for c in f_cols], axis=1), dtype=torch.float32)
    return xyz, quats, scales, opacities, features

# Scale, Quat â†’ Covariance ë³€í™˜ (ë°°ì¹˜)
def scale_quat_to_cov(scales: torch.Tensor, quats: torch.Tensor) -> torch.Tensor:
    """Scaleê³¼ Quaternionìœ¼ë¡œë¶€í„° ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°: Î£ = R @ diag(s^2) @ R^T"""
    q = quats / (quats.norm(dim=-1, keepdim=True) + 1e-9)
    qw, qx, qy, qz = q[..., 0], q[..., 1], q[..., 2], q[..., 3]
    R = torch.stack([
        1 - 2*(qy*qy + qz*qz), 2*(qx*qy - qw*qz), 2*(qx*qz + qw*qy),
        2*(qx*qy + qw*qz), 1 - 2*(qx*qx + qz*qz), 2*(qy*qz - qw*qx),
        2*(qx*qz - qw*qy), 2*(qy*qz + qw*qx), 1 - 2*(qx*qx + qy*qy)
    ], dim=-1).reshape(*q.shape[:-1], 3, 3)
    S_sq = torch.diag_embed(scales ** 2)
    return R @ S_sq @ R.transpose(-1, -2)

# í´ëŸ¬ìŠ¤í„°ë³„ Covariance ì—­ê³„ì‚° (ê°€ì¤‘ ê³µë¶„ì‚° + primitive ê³µë¶„ì‚°)
def compute_cluster_covariance(means: torch.Tensor, labels: torch.Tensor, centers: torch.Tensor, weights: torch.Tensor, scales: torch.Tensor, quats: torch.Tensor, min_variance: float = 1e-6):
    """
    ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì ë“¤ì˜ ë¶„í¬ë¡œë¶€í„° ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚° (Moment Matching)
    Î£_cluster = Î£_i w_i * [(Î¼_i - Î¼_c)(Î¼_i - Î¼_c)^T + Î£_i] / Î£_i w_i

    Args:
        means: (N, 3) ì›ë³¸ gaussian means
        labels: (N,) í´ëŸ¬ìŠ¤í„° í• ë‹¹
        centers: (K, 3) í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬
        weights: (N,) ê°€ì¤‘ì¹˜
        scales: (N, 3) ì›ë³¸ gaussian scales
        quats: (N, 4) ì›ë³¸ gaussian quaternions
        min_variance: ìµœì†Œ ë¶„ì‚° (ìˆ˜ì¹˜ ì•ˆì •ì„±)

    Returns:
        covs: (K, 3, 3) ê° í´ëŸ¬ìŠ¤í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ (ì–‘ì •ì¹˜ ë³´ì¥)
    """
    K = centers.shape[0]
    device = means.device
    covs = torch.zeros(K, 3, 3, device=device)
    labels_long = labels.long()

    # ëª¨ë“  primitiveì˜ ê³µë¶„ì‚° ë¯¸ë¦¬ ê³„ì‚°
    prim_covs = scale_quat_to_cov(scales, quats)  # (N, 3, 3)

    for k in range(K):
        mask = (labels_long == k)
        n_points = mask.sum().item()

        if n_points == 0:
            # ë¹ˆ í´ëŸ¬ìŠ¤í„°: identity * min_variance
            covs[k] = torch.eye(3, device=device) * min_variance
            continue

        # ê°€ì¤‘ ê³µë¶„ì‚° ê³„ì‚°
        sub_means = means[mask]  # (n, 3)
        sub_weights = weights[mask]  # (n,)
        sub_prim_covs = prim_covs[mask]  # (n, 3, 3)
        center = centers[k]  # (3,)
        w_sum = sub_weights.sum()

        if n_points == 1:
            # ë‹¨ì¼ ì : primitive ìì²´ì˜ ê³µë¶„ì‚° ì‚¬ìš©
            covs[k] = sub_prim_covs[0]
            continue

        # ì¤‘ì‹¬ìœ¼ë¡œë¶€í„°ì˜ í¸ì°¨
        diff = sub_means - center.unsqueeze(0)  # (n, 3)

        # ìœ„ì¹˜ ë¶„ì‚°: Î£_i w_i * (x_i - Î¼)(x_i - Î¼)^T / Î£_i w_i
        weighted_diff = diff * sub_weights.unsqueeze(1).sqrt()  # (n, 3)
        pos_cov = (weighted_diff.T @ weighted_diff) / w_sum  # (3, 3)

        # Primitive ê³µë¶„ì‚°ì˜ ê°€ì¤‘ í‰ê· : Î£_i w_i * Î£_i / Î£_i w_i
        weighted_prim_covs = sub_prim_covs * sub_weights.view(-1, 1, 1)  # (n, 3, 3)
        avg_prim_cov = weighted_prim_covs.sum(dim=0) / w_sum  # (3, 3)

        # Moment Matching: ìœ„ì¹˜ ë¶„ì‚° + primitive ê³µë¶„ì‚°
        cov = pos_cov + avg_prim_cov

        # ëŒ€ì¹­í™”
        cov = (cov + cov.T) / 2

        # ì–‘ì •ì¹˜ ë³´ì¥ (eigenvalue clamping)
        eigenvalues, eigenvectors = torch.linalg.eigh(cov)
        eigenvalues = eigenvalues.clamp(min=min_variance)
        cov = eigenvectors @ torch.diag(eigenvalues) @ eigenvectors.T

        covs[k] = cov

    return covs

# NPZ ì €ì¥ (Format 4.0)
def save_tensors_to_npz(out_path: Path, hierarchy, K: int, depth: int, primitive_labels: torch.Tensor):
    """Hierarchy ë°ì´í„°ë¥¼ NPZë¡œ ì €ì¥ (Format 4.0)"""
    save_dict = {
        'format_version': '4.0',
        'K': K,
        'depth': depth,
        'primitive_labels': primitive_labels.cpu().numpy(),
    }

    for level_idx, (lvl_mu, lvl_scales, lvl_quats, lvl_w, lvl_rgb) in enumerate(hierarchy):
        save_dict[f'level_{level_idx}_means'] = lvl_mu.cpu().numpy()
        save_dict[f'level_{level_idx}_scales'] = lvl_scales.cpu().numpy()
        save_dict[f'level_{level_idx}_quats'] = lvl_quats.cpu().numpy()
        save_dict[f'level_{level_idx}_weights'] = lvl_w.cpu().numpy()
        save_dict[f'level_{level_idx}_rgb'] = lvl_rgb.cpu().numpy()

    np.savez(str(out_path), **save_dict)
    print(f"âœ… Saved {depth} levels (K={K}, format=4.0) to {out_path}")

# ë©”ì¸ í•¨ìˆ˜
def reduce_scene(scene: str, branching_factor: int = 8, depth: int = 1, max_iters: int = 20, min_variance: float = 1e-6, tileB: int = 64, seed: int = 0, distance_metric: str = 'euclidean'):
    # Scene íŒŒì‹±
    parts = scene.split('/')
    if len(parts) == 2:
        dataset, scene_name = parts
    elif len(parts) == 1:
        scene_name = parts[0]
        dataset = scene_name
    else:
        raise ValueError(f"Scene must be 'scene' or 'dataset/scene', got: {scene}")

    # ê²½ë¡œ ì„¤ì •
    in_path = Path('/data/wgsong/dataset') / dataset / scene_name / 'gs.ply'
    out_dir = Path('/data/wgsong/dataset') / dataset / scene_name / 'reduced'
    out_dir.mkdir(parents=True, exist_ok=True)
    if not in_path.exists():
        raise FileNotFoundError(f"ì…ë ¥ PLYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {in_path}")

    # PLY ë¡œë“œ
    print(f"ğŸ“‚ Loading {in_path}")
    means, quats, scales, opacities, features = load_ply_as_tensors(in_path)
    means = means.cuda().contiguous()
    quats = quats.cuda().contiguous()
    scales = scales.cuda().contiguous()
    opacities = opacities.cuda().contiguous()
    features = features.cuda().contiguous()
    total_points = means.shape[0]

    # ê°€ì¤‘ì¹˜ ê³„ì‚° (opacity ê¸°ë°˜)
    opacities_activated = torch.sigmoid(opacities).view(-1)
    weights = opacities_activated.contiguous()

    # Hierarchical K-means
    import time
    final_k = branching_factor ** depth

    if distance_metric == 'euclidean':
        print(f"âš™ï¸ {total_points} â†’ {final_k} í´ëŸ¬ìŠ¤í„°ë¡œ ì¶•ì†Œ ì¤‘ (Euclidean K-means, means-based)...")
        print(f"ğŸ”„ Running BFS hierarchical K-means (K={branching_factor}, depth={depth}, max_iters={max_iters})...")
        start_time = time.time()

        # Euclidean K-means í˜¸ì¶œ (meansë§Œ ì‚¬ìš©)
        cluster_result = clustering_lib.clustering_euclidean_bfs(
            means,
            weights=weights,
            K=branching_factor,
            depth=depth,
            max_iters=max_iters,
            tol=1e-4,
        )

        elapsed = time.time() - start_time
        print(f"âœ… Clustering completed in {elapsed:.2f}s ({depth} levels)")

        # ê²°ê³¼ ì¶”ì¶œ
        primitive_labels = cluster_result['primitive_labels']  # (N,) int32
        level_mu = cluster_result['level_mu']  # list of (K^lvl, 3)
        level_cov = None  # Euclidean ëª¨ë“œì—ì„œëŠ” ìˆ˜ë™ ê³„ì‚° í•„ìš”
        level_w = None

    elif distance_metric == 'w2':
        # Primitive covariance ê³„ì‚°
        print(f"ğŸ”§ Computing primitive covariances...")
        covs = scale_quat_to_cov(scales, quats).contiguous()

        print(f"âš™ï¸ {total_points} â†’ {final_k} í´ëŸ¬ìŠ¤í„°ë¡œ ì¶•ì†Œ ì¤‘ (W2 K-means, CUDA accelerated)...")
        print(f"ğŸ”„ Running BFS hierarchical K-means (K={branching_factor}, depth={depth}, max_iters={max_iters})...")
        start_time = time.time()

        # CUDA ì»¤ë„ í˜¸ì¶œ (clustering_bfs)
        cluster_result = clustering_lib.clustering_bfs(
            means, covs,
            weights=weights,
            K=branching_factor,
            depth=depth,
            max_iters=max_iters,
            tol=1e-4,
            tileB=tileB,
            seed=seed,
            reseed_empty=True
        )

        elapsed = time.time() - start_time
        print(f"âœ… Clustering completed in {elapsed:.2f}s ({depth} levels)")

        # ê²°ê³¼ ì¶”ì¶œ
        primitive_labels = cluster_result['primitive_labels']  # (N,) int32
        level_mu = cluster_result['level_mu']  # list of (K^lvl, 3)
        level_cov = cluster_result['level_cov']  # list of (K^lvl, 3, 3)
        level_w = cluster_result['level_w']  # list of (K^lvl,)

    else:
        raise ValueError(f"Invalid distance_metric: {distance_metric}. Must be 'euclidean' or 'w2'.")

    # RGB ì¤€ë¹„
    features_dc = features[:, :3]
    features_rgb = SH2RGB(features_dc)
    weighted_rgb = features_rgb * weights.unsqueeze(1)
    two_pi_pow = (2.0 * 3.141592653589793) ** (3.0 / 2.0)

    # ê° ë ˆë²¨ ì²˜ë¦¬
    print(f"ğŸ”§ Processing {depth} levels...")
    hierarchy_processed = []

    for level_idx in range(depth):
        lvl_centers = level_mu[level_idx]  # (K^lvl, 3)
        lvl_k = lvl_centers.shape[0]

        # Primitive â†’ Level ë§¤í•‘ ê³„ì‚° (implicit tree êµ¬ì¡°)
        if level_idx == depth - 1:
            lvl_labels_long = primitive_labels.long()
        else:
            divisor = branching_factor ** (depth - 1 - level_idx)
            lvl_labels_long = (primitive_labels // divisor).long()

        # Covariance ë° Scale/Quaternion ê³„ì‚°
        if distance_metric == 'euclidean':
            # Euclidean ëª¨ë“œ: ìˆ˜ë™ìœ¼ë¡œ covariance ê³„ì‚°
            lvl_cov = compute_cluster_covariance(means, lvl_labels_long, lvl_centers, weights, scales, quats, min_variance)
            # CUDA ì»¤ë„ë¡œ ë³€í™˜
            lvl_scales, lvl_quats = clustering_lib.cov_to_scale_quat(lvl_cov)
        else:  # w2
            # W2 ëª¨ë“œ: ì´ë¯¸ ê³„ì‚°ëœ covariance ì‚¬ìš©
            lvl_cov = level_cov[level_idx]  # (K^lvl, 3, 3)
            lvl_scales, lvl_quats = clustering_lib.cov_to_scale_quat(lvl_cov)

        # RGB ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        lvl_rgb = torch.zeros(lvl_k, 3, device=features.device, dtype=features.dtype)
        lvl_rgb_weight = torch.zeros(lvl_k, device=features.device, dtype=torch.float32)
        lvl_index = lvl_labels_long.unsqueeze(1).expand_as(weighted_rgb)
        lvl_rgb.scatter_add_(0, lvl_index, weighted_rgb)
        lvl_rgb_weight.scatter_add_(0, lvl_labels_long, weights)
        lvl_nonzero = lvl_rgb_weight > 0
        lvl_rgb[lvl_nonzero] /= lvl_rgb_weight[lvl_nonzero].unsqueeze(1)

        # Weight ê³„ì‚° (opacity sum * sqrt(det(cov)))
        lvl_opacity_sum = torch.zeros(lvl_k, device=opacities_activated.device, dtype=torch.float32)
        lvl_opacity_sum.scatter_add_(0, lvl_labels_long, opacities_activated)
        lvl_det = torch.det(lvl_cov).clamp(min=1e-10)
        lvl_sqrt_det = torch.sqrt(lvl_det)
        lvl_w = lvl_opacity_sum * two_pi_pow * lvl_sqrt_det

        hierarchy_processed.append((lvl_centers, lvl_scales, lvl_quats, lvl_w, lvl_rgb))
        print(f"   Level {level_idx}: {lvl_k} clusters, cov min={lvl_cov.min().item():.6f}, max={lvl_cov.max().item():.6f}")

    # íŒŒì¼ëª… ê²°ì •
    actual_k = hierarchy_processed[-1][0].shape[0]
    metric_suffix = "euc" if distance_metric == 'euclidean' else "w2"
    if depth > 1:
        filename_base = f"{actual_k}_h{branching_factor}x{depth}_{metric_suffix}"
    else:
        filename_base = f"{actual_k}_{metric_suffix}"

    # NPZ ì €ì¥
    out_path_npz = out_dir / f"{filename_base}.npz"
    save_tensors_to_npz(out_path_npz, hierarchy_processed, branching_factor, depth, primitive_labels.int())
    print(f"ğŸ“ Output: {out_path_npz}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Hierarchical K-meansë¡œ Gaussian Splatting PLY ì¶•ì†Œ (Euclidean/W2 ê±°ë¦¬ ì§€ì›)")
    parser.add_argument("--scene", type=str, required=True, help="Scene: 'dataset/scene' (ì˜ˆ: 'tank-temp/truck')")
    parser.add_argument("--branching_factor", type=int, default=8, help="ê° ë ˆë²¨ì—ì„œ ë¶„í• í•  í´ëŸ¬ìŠ¤í„° ìˆ˜")
    parser.add_argument("--depth", type=int, default=1, help="ë¶„í•  ê¹Šì´ (ìµœì¢… í´ëŸ¬ìŠ¤í„° ìˆ˜ = branching_factor^depth)")
    parser.add_argument("--max_iters", type=int, default=20, help="K-means ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜")
    parser.add_argument("--min_variance", type=float, default=1e-6, help="ìµœì†Œ ë¶„ì‚° (ê³µë¶„ì‚° ì •ê·œí™”)")
    parser.add_argument("--distance_metric", type=str, default='euclidean', choices=['euclidean', 'w2'],
                        help="ê±°ë¦¬ ë©”íŠ¸ë¦­: 'euclidean' (means ê¸°ë°˜, ê¸°ë³¸ê°’) ë˜ëŠ” 'w2' (Wasserstein-2, CUDA)")
    parser.add_argument("--tileB", type=int, default=64, help="CUDA tile size for shared memory (W2 ëª¨ë“œë§Œ)")
    parser.add_argument("--seed", type=int, default=0, help="Random seed for FPS initialization (W2 ëª¨ë“œë§Œ)")
    args = parser.parse_args()

    reduce_scene(
        args.scene,
        branching_factor=args.branching_factor,
        depth=args.depth,
        max_iters=args.max_iters,
        min_variance=args.min_variance,
        tileB=args.tileB,
        seed=args.seed,
        distance_metric=args.distance_metric
    )
